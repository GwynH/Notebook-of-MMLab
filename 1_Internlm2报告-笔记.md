# Internlm2报告-笔记

How to effectively extend the context length of LLMs is currently a hot research topic.

InternLM2 achieves commendable performance in the "Needle-in-a-Haystack" test within 200k contexts.


## COnditional OnLine RLHF (COOL RLHF)

COOL RLHF adopts a conditional reward model to reconcile diverse but potentially conflicting preferences and executes Proximal Policy Optimization (PPO) over multiple rounds to mitigate emerging reward hacking in each phase.


整理中……
